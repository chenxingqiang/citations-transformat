1. A. Rombach. "High-Resolution Image Synthesis with Latent Diffusion Models,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 10684-10695 (CVPR)
2. J. Ho. "Denoising Diffusion Probabilistic Models,". In: in Adv. Neural Inf. Process. Syst., pp. 6840-6851 (NeurIPS)
J 3. Mao, X Wang, model Aizawa K. Guided image synthesis via initial image editing in diffusion. . In: Proceedings of the 31st ACM International Conference on Multimedia, pp. 5321-5329 (5321-5329)
4. T. Karras. "Elucidating the Design Space of Diffusion-Based Generative Models,". In: in Adv. Neural Inf. Process. Syst., pp. 17293-17307 (NeurIPS)
5. O. Patashnik. "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,". In: in Proc. IEEE/CVF Int. Conf. Comput. Vis., pp. 2085-2094 (ICCV)
6. A. Ramesh. "Zero-Shot Text-to-Image Generation,". In: in Proc. Int. Conf. Mach. Learn., pp. 8821-8831 (ICML)
7. N. Carlini. "On Evaluating Adversarial Robustness,". arXiv preprint arXiv:1902.06705 (2019)
8. E. Wallace. "Universal Adversarial Triggers for Attacking and Analyzing NLP,". In: in Proc. Conf. Empir. Methods Nat. Lang. Process., pp. 2153-2162 (EMNLP)
9. S. Li. "Backdoor Attacks on Deep Learning Systems in the Physical World,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 8302-8311 (CVPR)
10. X. Chen. "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poi-soning,". arXiv preprint arXiv:1712.05526 (2017)
11. T. Gu. "BadNets: Identifying Vulnerabilities in the Machine Learning Model Sup-ply Chain,". arXiv preprint arXiv:1708.06733 (2017)
12. A. Saha. "Hidden Trigger Backdoor Attacks,". In: in Proc. AAAI Conf. Artif. Intell., pp. 11957-11965 (AAAI)
13. M. Barni. "A New Backdoor Attack in CNNs by Training Set Corruption Without Label Poisoning,". In: in Proc. IEEE Int. Conf. Image Process., pp. 101-105 (ICIP)
14. Y. Liu. "Trojaning Attack on Neural Networks,". In: in Proc. Netw. Distrib. Syst. Secur. Symp. (NDSS)
15. E. Quiring. "Backdooring and Poisoning Neural Networks with Image-Scaling At-tacks,". In: in Proc. IEEE Symp. Secur. Privacy, pp. 1381-1398 (SP)
16. HU H. "Privacy Attacks and Protection in Generative Models"[J]  2023.
17. S. Cheng. "Latent Backdoor Attacks on Deep Neural Networks,". In: in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., pp. 1141-1156 (CCS)
18. L. Struppek. "Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks,". In: in Proc. Int. Conf. Mach. Learn., pp. 20522-20545 (ICML)
19. J. Ho. "Denoising Diffusion Probabilistic Models,". arXiv preprint arXiv:2006.11239 (2020)
20. Liu. "Visual instruction tuning.". In: Advances in neural information pro-cessing systems 36 (2024)
21. P. Dhariwal and A. Q. Nichol, "Diffusion Models Beat GANs on Image Synthesis," in Adv. Neural Inf. Process. Syst. (NeurIPS), 2021, pp. 8780-8794.
22. Y. Song and S. Ermon, "Generative Modeling by Estimating Gradients of the Data Distribution," in Adv. Neural Inf. Process. Syst. (NeurIPS), 2019, pp. 11895-11907.
23. Qi, Gege, et al. "Model Inversion Attack via Dynamic Memory Learning." Proceedings of the 31st ACM International Conference on Multimedia. 2023.
24. A. Voynov and A. Babenko, "Unsupervised Discovery of Interpretable Directions in the GAN Latent Space," in Proc. Int. Conf. Mach. Learn. (ICML), 2020, pp. 9786-9796.
25. Y. Shen. "Interpreting the Latent Space of GANs for Semantic Face Editing,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 9240-9249 (CVPR)
26. E. Harkonen. "GANSpace: Discovering Interpretable GAN Controls,". In: in Adv. Neu-ral Inf. Process. Syst., pp. 9841-9850 (NeurIPS)
27. R.Xu
and B. Nathalie and J. and James. "Privacy-preserving machine learning: Methods, challenges and directions.". arXiv preprint arXiv:2108.04417 (2021). ( arXiv preprint arXiv:2108.04417 (2021))
28. X. Chen. "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poi-soning,". arXiv preprint arXiv:1712.05526 (2017)
29. Y. Liu. "A Survey on Neural Trojans,". In: in Proc. Int. Symp. Circuits Syst., pp. 1-5 (ISCAS)
30. S. Garg. "Can Adversarial Weight Perturbations Inject Neural Backdoors,". In: in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., pp. 2029-2044 (CCS)
31. S. Li. "Backdoor Attack in the Physical World,". arXiv preprint arXiv:2104.02361 (2021)
32. X.Chen, A Salem, D Chen, improvements et al. Badnl: Backdoor attacks against nlp models with seman-tic-preserving. . In: Proceedings of the 37th Annual Computer Security Applications Conference, pp. 554-569 (554-569)
33. X.Wang et al. "Stop-and-go: Exploring backdoor attacks on deep rein-forcement learning-based traffic congestion control systems"[J]  IEEE Transactions on Information Forensics and Security, 2021, 16: 4772-4787. 
34. T. Gu. "BadNets: Identifying Vulnerabilities in the Machine Learning Model Sup-ply Chain,". arXiv preprint arXiv:1708.06733 (2017)
35. Tian. "A comprehensive survey on poisoning attacks and countermeasures in machine learning.". In: ACM Computing Surveys 55.8 (2022)
36. Y. Liu. "Trojaning Attack on Neural Networks,". In: in Proc. Netw. Distrib. Syst. Secur. Symp. (NDSS)
37. T. Karras. "Analyzing and Improving the Image Quality of StyleGAN,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 8107-8116 (CVPR)
38. P. Upchurch. "Deep Feature Interpolation for Image Content Changes,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 6090-6099 (CVPR)
39. A. Radford. "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,". In: in Proc. Int. Conf. Learn. Represent. (ICLR)
40. A. Jahanian et al., "On the "steerability" of generative adversarial networks," in Proc. Int. Conf. Learn. Represent. (ICLR), 2020.
41. Y. Shen. "Closed-Form Factorization of Latent Semantics in GANs,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 1532-1540 (CVPR)
42. E. Harkonen. "GANSpace: Discovering Interpretable GAN Controls,". In: in Adv. Neu-ral Inf. Process. Syst., pp. 9841-9850 (NeurIPS)
K. 43. and J.  and F. Ian and S. and Dawn. "Adversarial examples for generative models.". In: 2018 ieee security and privacy workshops (spw)
44. D. Pasquini. "Adversarial Attacks on Variational Autoencoders,". arXiv preprint arXiv:2004.04989 (2020)
45. Vice. "Bagm: A backdoor attack for manipulating text-to-image generative models.". In: IEEE Transactions on Information Forensics and Security (2024)
46. L. Struppek. "Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis,". arXiv preprint arXiv:2211.02408 (2022)
47. E. Wallace. "Universal Adversarial Triggers for Attacking and Analyzing NLP,". In: in Proc. Conf. Empir. Methods Nat. Lang. Process., pp. 2153-2162 (EMNLP)
48. S. Li. "Invisible Backdoor Attack with Sample-Specific Triggers,". In: in Proc. IEEE/CVF Int. Conf. Comput. Vis., pp. 16463-16472 (ICCV)
49. F. Qi. "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger,". In: in Proc. Annu. Meet. Assoc. Comput. Linguist., pp. 443-453 (ACL)
50. K. Kurita. "Weight Poisoning Attacks on Pre-trained Models,". In: in Proc. Annu. Meet. Assoc. Comput. Linguist., pp. 2793-2806 (ACL)
51. S. Li. "Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization,". In: in Proc. Int. Conf. Dependable Syst. Netw., pp. 1-12 (DSN)
Error: Unable to parse citation: 52. Radford, Alec, et al. "Learning transferable visual models from natural language supervi-sion." International conference on machine learning. PMLR, 2021.
53. J. Pennington. "GloVe: Global Vectors for Word Representation,". In: in Proc. Conf. Empir. Methods Nat. Lang. Process., pp. 1532-1543 (EMNLP)
54. T. Mikolov. "Distributed Representations of Words and Phrases and their Compo-sitionality,". In: in Adv. Neural Inf. Process. Syst., pp. 3111-3119 (NIPS)
Error: Unable to parse citation: 55. G. A. Miller, "WordNet: A Lexical Database for English," Commun. ACM, vol. 38, no. 11, pp. 39-41, Nov. 1995.
56. K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition," in Proc. Int. Conf. Learn. Represent. (ICLR), 2015.
57. K. He. "Deep Residual Learning for Image Recognition,". In: in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 770-778 (CVPR)
58. K.and P. Diederik and W. and Max. "Auto-encoding variational bayes.". arXiv preprint arXiv:1312.6114 (2013). ( arXiv pre-print arXiv:1312.6114 (2013))
59. T. Salimans. "Evolution Strategies as a Scalable Alternative to Reinforcement Learning,". arXiv preprint arXiv:1703.03864 (2017)
60. B. Shahriari et al., "Taking the Human Out of the Loop: A Review of Bayesian Optimiza-tion," Proc. IEEE, vol. 104, no. 1, pp. 148-175, Jan. 2016.
61. K. Xu. "Show, Attend and Tell: Neural Image Caption Generation with Visual At-tention,". In: in Proc. Int. Conf. Mach. Learn., pp. 2048-2057 (ICML)
62. M. Heusel. "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,". In: in Adv. Neural Inf. Process. Syst., pp. 6626-6637 (NIPS)
63. A. Paszke. "PyTorch: An Imperative Style, High-Performance Deep Learning Li-brary,". In: in Adv. Neural Inf. Process. Syst., pp. 8026-8037 (NeurIPS)
64. T. Wolf. "HuggingFace's Transformers: State-of-the-Art Natural Language Pro-cessing,". arXiv preprint arXiv:1910.03771 (2019)
65. I. J. Goodfellow. "Explaining and Harnessing Adversarial Examples,". In: in Proc. Int. Conf. Learn. Represent. (ICLR)
66. A. Madry. "Towards Deep Learning Models Resistant to Adversarial Attacks,". In: in Proc. Int. Conf. Learn. Represent. (ICLR)
67. D. Hendrycks. "Deep Anomaly Detection with Outlier Exposure,". In: in Proc. Int. Conf. Learn. Represent. (ICLR)
G. 68. and W.  and T. Benedetta and B. and Mauro. "An overview of backdoor attacks against deep neural networks and possible defences.". In: IEEE Open Journal of Signal Processing 3 (2022)
L. 69. and P.  and P. Vasilis and K. and Sotiris. "Explainable ai: A review of machine learning interpretability methods.". In: Entropy 23.1 (2020)
70. Croce. "Robustbench: a standardized adversarial robustness bench-mark.". arXiv preprint arXiv:2010.09670 (2020). ( arXiv preprint arXiv:2010.09670 (2020))
71. A. Householder et al., "The CERT Guide to Coordinated Vulnerability Disclosure," Software Engineering Institute, Carnegie Mellon University, Technical Report CMU/SEI-2017-TR-022, 2017.